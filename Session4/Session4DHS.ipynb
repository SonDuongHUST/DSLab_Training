{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session4DHS.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNggG45scs6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "from collections import defaultdict\n",
        "from os import listdir\n",
        "from os.path import isfile\n",
        "import random\n",
        "\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_eager_execution()\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QP5xB31Bc_Nj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MAX_DOC_LENGTH = 500  # do dai lon nhat cua 1 document\n",
        "NUM_CLASSES = 20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Onz4pPBqdVC6",
        "colab_type": "code",
        "outputId": "e108a9bb-1cb0-4dd0-9b51-7afb50e3f53b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ek8kH6OCdSuj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Thu thap du lieu tu files va xay dung tu dien\n",
        "def gen_data_and_vocab():\n",
        "    def collect_data_from(parent_path, newsgroup_list, word_count=None):\n",
        "        data = []  # chua noi dung cac van ban\n",
        "        for group_id, newsgroup in enumerate(newsgroup_list):\n",
        "            dir_path = parent_path + '/' + newsgroup + '/'\n",
        "            files = [(filename, dir_path + filename)\n",
        "                     for filename in listdir(dir_path)\n",
        "                     if isfile(dir_path + filename)]\n",
        "            files.sort()  # sap xep theo filename\n",
        "            label = group_id\n",
        "            print('Processing: {}-{}'.format(group_id, newsgroup))\n",
        "            for filename, filepath in files:\n",
        "                with open(filepath, 'rb') as f:\n",
        "                    text= f.read().decode('UTF-8', errors='ignore').lower()\n",
        "                    words = re.split('\\W+', text)\n",
        "                    if word_count is not None: # chi ap dung cho train data \n",
        "                        for word in words:\n",
        "                            word_count[word] += 1\n",
        "                    content = ' '.join(words)\n",
        "                    assert len(content.splitlines()) == 1\n",
        "                    data.append(str(label) + '<fff>' + filename + '<fff>' + content)\n",
        "        return data\n",
        "  \n",
        "    word_count = defaultdict(int)\n",
        "    path = '/content/drive/My Drive/DSLabTraining/Datasets/20news-bydate/'\n",
        "    parts = [path + dir_name + '/' for dir_name in listdir(path)\n",
        "             if not isfile(path + dir_name)]\n",
        "    train_path, test_path = (parts[0], parts[1]) \\\n",
        "        if 'train' in parts[0] else (parts[1], parts[0])\n",
        "    newsgroup_list = [newsgroup for newsgroup in listdir(train_path)]\n",
        "    newsgroup_list.sort()\n",
        "\n",
        "    train_data = collect_data_from(\n",
        "        parent_path=train_path,\n",
        "        newsgroup_list=newsgroup_list,\n",
        "        word_count=word_count\n",
        "    )\n",
        "    # tu dien bao gom nhung tu co tan suat xuat hien du lon\n",
        "    vocab = [word for word, fred in\n",
        "             zip(word_count.keys(), word_count.values()) if fred > 10]\n",
        "    vocab.sort()\n",
        "    with open('/content/drive/My Drive/DSLabTraining/Datasets/w2v/vocab-raw.txt', 'w') as f:\n",
        "        f.write('\\n'.join(vocab))\n",
        "\n",
        "    test_data = collect_data_from(\n",
        "        parent_path=test_path,\n",
        "        newsgroup_list=newsgroup_list\n",
        "    )\n",
        "\n",
        "    with open('/content/drive/My Drive/DSLabTraining/Datasets/w2v/20news-train-raw.txt', 'w') as f:\n",
        "        f.write('\\n'.join(train_data))\n",
        "    with open('/content/drive/My Drive/DSLabTraining/Datasets/w2v/20news-test-raw.txt', 'w') as f:\n",
        "        f.write('\\n'.join(test_data))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNb_tlK6VIJp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# encode du lieu\n",
        "# Ma hoa moi van ban boi ID cua cac tu co trong van ban do\n",
        "def encode_data(data_path, vocab_path):  # data_path chi den file 20news-train-raw.txt va file 20news-test-raw\n",
        "    with open(vocab_path) as f:\n",
        "        vocab = dict([(word, word_ID + 2)\n",
        "                      for word_ID, word in enumerate(f.read().splitlines())])\n",
        "    unknown_ID = 0  # ID cua tu khong xuat hien trong vocab\n",
        "    padding_ID = 1  # ID cua tu rong trong cac van ban\n",
        "    with open(data_path) as f:\n",
        "        documents = [(line.split('<fff>')[0], line.split('<fff>')[1], line.split('<fff>')[2])\n",
        "                     for line in f.read().splitlines()]\n",
        "    encoded_data = []\n",
        "    # lay thong tin tu cac van ban\n",
        "    for document in documents:\n",
        "        label, doc_id, text = document\n",
        "        words = text.split()[:MAX_DOC_LENGTH]\n",
        "        sentence_length = len(words)\n",
        "        encoded_text = []\n",
        "\n",
        "        for word in words:\n",
        "            if word in vocab:\n",
        "                encoded_text.append(str(vocab[word]))\n",
        "            else:\n",
        "                encoded_text.append(str(unknown_ID))\n",
        "        if sentence_length < MAX_DOC_LENGTH:\n",
        "            num_padding = MAX_DOC_LENGTH - sentence_length\n",
        "            for _ in range(num_padding):\n",
        "                encoded_text.append(str(padding_ID))\n",
        "        encoded_data.append(str(label) + '<fff>' + str(doc_id) + '<fff>' + str(sentence_length)\n",
        "                            + '<fff>' + ' '.join(encoded_text))\n",
        "    dir_name = '/'.join(data_path.split('/')[:-1])\n",
        "    file_name = '-'.join(data_path.split('/')[-1].split('-')[:-1]) + '-encoded.txt'\n",
        "    with open(dir_name + '/' + file_name, 'w') as f:\n",
        "        f.write('\\n'.join(encoded_data))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_r7GD5bV6ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DataReader:\n",
        "    def __init__(self, data_path, batch_size):\n",
        "        self._batch_size = batch_size\n",
        "        with open(data_path) as f:\n",
        "            d_lines = f.read().splitlines()\n",
        "        self._data = []\n",
        "        self._labels = []\n",
        "        self._sentence_lengths = []\n",
        "        for _, line in enumerate(d_lines):\n",
        "          if len(line) > 1:\n",
        "            features = line.split(\"<fff>\")\n",
        "            label, sentence_length = int(features[0]), int(features[2])\n",
        "            tokens = features[3].split()\n",
        "            vector = [int(token) for token in tokens]\n",
        "            self._data.append(vector)\n",
        "            self._labels.append(label)\n",
        "            self._sentence_lengths.append(sentence_length)\n",
        "        self._data = np.array(self._data)\n",
        "        self._labels = np.array(self._labels)\n",
        "        self._sentence_lengths = np.array(self._sentence_lengths)\n",
        "        \n",
        "        self._num_epoch = 0\n",
        "        self._batch_id = 0\n",
        "\n",
        "    def next_batch(self):\n",
        "        start = self._batch_id * self._batch_size\n",
        "        end = start + self._batch_size\n",
        "        self._batch_id += 1\n",
        "\n",
        "        if end + self._batch_size > len(self._data):\n",
        "            end = len(self._data)\n",
        "            start = end - self._batch_size\n",
        "            self._num_epoch += 1\n",
        "            self._batch_id = 0\n",
        "            indices = list(range(len(self._data)))\n",
        "            random.seed(2020)\n",
        "            random.shuffle(indices)\n",
        "            self._data, self._labels, self._sentence_lengths= self._data[indices], self._labels[indices], self._sentence_lengths[indices]\n",
        "\n",
        "        return self._data[start:end], self._labels[start:end], self._sentence_lengths[start:end]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hl56pDO1W6WH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN:\n",
        "    def __init__(self, vocab_size, embedding_size, lstm_size, batch_size):\n",
        "        self._vocab_size = vocab_size\n",
        "        self._embedding_size = embedding_size\n",
        "        self._lstm_size = lstm_size\n",
        "        self._batch_size = batch_size\n",
        "\n",
        "        self._data = tf.placeholder(tf.int32, shape=[batch_size, MAX_DOC_LENGTH])\n",
        "        self._labels = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
        "        self._sentence_lengths = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
        "        # self._final_tokens = tf.placeholder(tf.int32, shape=[batch_size, ])\n",
        "\n",
        "    def build_graph(self):\n",
        "        embeddings = self.embedding_layer(self._data)\n",
        "        lstm_outputs = self.LSTM_layer(embeddings)\n",
        "        \n",
        "        with tf.variable_scope(\"rnn_variables\", reuse= tf.AUTO_REUSE) as scope:\n",
        "          weights = tf.get_variable(\n",
        "              name='final_layer_weights',\n",
        "              shape=(self._lstm_size, NUM_CLASSES),\n",
        "              initializer=tf.random_normal_initializer(seed=2020)\n",
        "          )\n",
        "          biases = tf.get_variable(\n",
        "              name='final_layer_biases',\n",
        "              shape=(NUM_CLASSES),\n",
        "              initializer=tf.random_normal_initializer(seed=2020)\n",
        "          )\n",
        "\n",
        "          logits = tf.matmul(lstm_outputs, weights) + biases\n",
        "          labels_one_hot = tf.one_hot(\n",
        "              indices=self._labels,\n",
        "              depth=NUM_CLASSES,\n",
        "              dtype=tf.float32\n",
        "          )\n",
        "          loss = tf.nn.softmax_cross_entropy_with_logits(\n",
        "              labels=labels_one_hot,\n",
        "              logits=logits\n",
        "          )\n",
        "          loss = tf.reduce_mean(loss)\n",
        "\n",
        "          probs = tf.nn.softmax(logits)\n",
        "          predicted_labels = tf.argmax(probs, axis=1)\n",
        "          predicted_labels = tf.squeeze(predicted_labels)\n",
        "\n",
        "        return predicted_labels, loss\n",
        "\n",
        "    def embedding_layer(self, indices):\n",
        "        pretrained_vectors = []\n",
        "        pretrained_vectors.append(np.zeros(self._embedding_size))\n",
        "        np.random.seed(2020)\n",
        "        for _ in range(self._vocab_size + 1):\n",
        "            pretrained_vectors.append(np.random.normal(loc=0., scale=1., size=self._embedding_size))\n",
        "\n",
        "        pretrained_vectors = np.array(pretrained_vectors)\n",
        "        with tf.variable_scope(\"rnn_variables\", reuse=tf.AUTO_REUSE) as scope:\n",
        "          self._embedding_matrix = tf.get_variable(\n",
        "              name='embedding',\n",
        "              shape=(self._vocab_size + 2, self._embedding_size),\n",
        "              initializer=tf.constant_initializer(pretrained_vectors)\n",
        "          )\n",
        "        return tf.nn.embedding_lookup(self._embedding_matrix, indices)\n",
        "\n",
        "    def LSTM_layer(self, embeddings):\n",
        "        lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(self._lstm_size)\n",
        "        zero_state = tf.zeros(shape=(self._batch_size, self._lstm_size))\n",
        "        initial_state = tf.nn.rnn_cell.LSTMStateTuple(zero_state, zero_state)\n",
        "\n",
        "        lstm_inputs = tf.unstack(tf.transpose(embeddings, perm=[1, 0, 2]))\n",
        "        lstm_outputs, last_state = tf.nn.static_rnn(\n",
        "            cell=lstm_cell,\n",
        "            inputs=lstm_inputs,\n",
        "            initial_state=initial_state,\n",
        "            sequence_length=self._sentence_lengths\n",
        "        )\n",
        "\n",
        "        lstm_outputs = tf.unstack(tf.transpose(lstm_outputs, perm=[1, 0, 2]))\n",
        "        lstm_outputs = tf.concat(\n",
        "            lstm_outputs,\n",
        "            axis=0\n",
        "        )  # [num docs * MAX_SENT_LENGTH,  ]\n",
        "\n",
        "        # self._mask : [num docs * MAX_SENT_LENGTH, ]\n",
        "        mask = tf.sequence_mask(\n",
        "            lengths=self._sentence_lengths,\n",
        "            maxlen=MAX_DOC_LENGTH,\n",
        "            dtype=tf.float32\n",
        "        )  # [num_docs, MAX_SENTENCE_LENGTH]\n",
        "        mask = tf.concat(tf.unstack(mask, axis=0), axis=0)\n",
        "        mask = tf.expand_dims(mask, -1)\n",
        "        lstm_outputs = mask * lstm_outputs\n",
        "        lstm_outputs_split = tf.split(lstm_outputs, num_or_size_splits=self._batch_size)\n",
        "        lstm_outputs_sum = tf.reduce_sum(lstm_outputs_split, axis=1)  # [num_docs, lsmt_size]\n",
        "        lstm_outputs_average = lstm_outputs_sum / tf.expand_dims(tf.cast(self._sentence_lengths, tf.float32), -1)\n",
        "        return lstm_outputs_average\n",
        "\n",
        "    def trainer(self, loss, learning_rate):\n",
        "        with tf.variable_scope(\"rnn_variables\", reuse= tf.AUTO_REUSE) as scope:\n",
        "          train_op = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
        "        return train_op\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMj6x4TFYko7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_and_evaluate_RNN():\n",
        "    with open('/content/drive/My Drive/DSLabTraining/Datasets/w2v/vocab-raw.txt') as f:\n",
        "        vocab_size = len(f.read().splitlines())\n",
        "    tf.set_random_seed(2020)\n",
        "    rnn = RNN(\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_size=300,\n",
        "        lstm_size=50,\n",
        "        batch_size=50\n",
        "    )\n",
        "    predicted_labels, loss = rnn.build_graph()\n",
        "    train_op = rnn.trainer(loss=loss, learning_rate=0.01)\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        train_data_reader = DataReader(\n",
        "            data_path='/content/drive/My Drive/DSLabTraining/Datasets/w2v/20news-train-encoded.txt',\n",
        "            batch_size=50\n",
        "        )\n",
        "        test_data_reader = DataReader(\n",
        "            data_path='/content/drive/My Drive/DSLabTraining/Datasets/w2v/20news-test-encoded.txt',\n",
        "            batch_size=50\n",
        "        )\n",
        "        step = 0\n",
        "        MAX_STEP = 2000\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "\n",
        "        while step < MAX_STEP:\n",
        "            next_train_batch = train_data_reader.next_batch()\n",
        "            train_data, train_labels, train_sentence_lengths = next_train_batch\n",
        "            plabels_eval, loss_eval, _ = sess.run(\n",
        "                [predicted_labels, loss, train_op],\n",
        "                feed_dict={\n",
        "                    rnn._data: train_data,\n",
        "                    rnn._labels: train_labels,\n",
        "                    rnn._sentence_lengths: train_sentence_lengths,\n",
        "                    # rnn._final_tokens: train_final_tokens\n",
        "                }\n",
        "            )\n",
        "            step += 1\n",
        "            if step % 50 == 0:\n",
        "                print('loss after step {} is {}'.format(step, loss_eval))\n",
        "\n",
        "            if train_data_reader._batch_id == 0:\n",
        "                num_true_preds = 0\n",
        "                while True:\n",
        "                    next_test_batch = test_data_reader.next_batch()\n",
        "                    test_data, test_labels, test_sentence_lengths = next_test_batch\n",
        "\n",
        "                    test_plabels_eval = sess.run(\n",
        "                        predicted_labels,\n",
        "                        feed_dict={\n",
        "                            rnn._data: test_data,\n",
        "                            rnn._labels: test_labels,\n",
        "                            rnn._sentence_lengths: test_sentence_lengths,\n",
        "                            # rnn._final_tokens: test_final_tokens\n",
        "                        }\n",
        "                    )\n",
        "                    matches = np.equal(test_plabels_eval, test_labels)\n",
        "                    num_true_preds += np.sum(matches.astype(float))\n",
        "\n",
        "                    if test_data_reader._batch_id == 0:\n",
        "                        break\n",
        "\n",
        "                print('Epoch:', train_data_reader._num_epoch)\n",
        "                print('Accuracy on test data:', num_true_preds * 100. / len(test_data_reader._data))\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mkrL0Hlaa2_",
        "colab_type": "code",
        "outputId": "20d4cbdb-f5e2-4a96-ae35-e6d4f1a6e729",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "if __name__==\"__main__\":\n",
        "    #gen_data_and_vocab()\n",
        "\n",
        "    #train_data_path='/content/drive/My Drive/DSLabTraining/Datasets/w2v/20news-train-raw.txt'\n",
        "    #test_data_path='/content/drive/My Drive/DSLabTraining/Datasets/w2v/20news-test-raw.txt'\n",
        "    #vocab_path='/content/drive/My Drive/DSLabTraining/Datasets/w2v/vocab-raw.txt'\n",
        "    #encode_data(train_data_path, vocab_path)\n",
        "    #encode_data(test_data_path, vocab_path)\n",
        "    \n",
        "    with tf.variable_scope(\"rnn_variables\", reuse= tf.AUTO_REUSE) as scope:\n",
        "        train_and_evaluate_RNN()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py:1666: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From <ipython-input-7-03fa5d62807d>:64: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-7-03fa5d62807d>:73: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:740: Layer.add_variable (from tensorflow.python.keras.engine.base_layer_v1) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:744: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-7-03fa5d62807d>:37: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "loss after step 50 is 2.987929344177246\n",
            "loss after step 100 is 1.4307098388671875\n",
            "loss after step 150 is 0.5380045175552368\n",
            "loss after step 200 is 5.457465171813965\n",
            "Epoch: 1\n",
            "Accuracy on test data: 7.221558476038763\n",
            "loss after step 250 is 2.7571749687194824\n",
            "loss after step 300 is 2.1142306327819824\n",
            "loss after step 350 is 1.833240032196045\n",
            "loss after step 400 is 1.4564995765686035\n",
            "loss after step 450 is 1.423457145690918\n",
            "Epoch: 2\n",
            "Accuracy on test data: 70.27744590468605\n",
            "loss after step 500 is 0.7744036912918091\n",
            "loss after step 550 is 0.7803292274475098\n",
            "loss after step 600 is 0.724478542804718\n",
            "loss after step 650 is 0.4168756604194641\n",
            "Epoch: 3\n",
            "Accuracy on test data: 75.5874153723616\n",
            "loss after step 700 is 0.3584956228733063\n",
            "loss after step 750 is 0.3508073091506958\n",
            "loss after step 800 is 0.27851372957229614\n",
            "loss after step 850 is 0.26135164499282837\n",
            "loss after step 900 is 0.4573529362678528\n",
            "Epoch: 4\n",
            "Accuracy on test data: 76.90163281561131\n",
            "loss after step 950 is 0.09467673301696777\n",
            "loss after step 1000 is 0.11358542740345001\n",
            "loss after step 1050 is 0.09132792800664902\n",
            "loss after step 1100 is 0.06205711513757706\n",
            "Epoch: 5\n",
            "Accuracy on test data: 76.66268418956591\n",
            "loss after step 1150 is 0.025837883353233337\n",
            "loss after step 1200 is 0.02224745787680149\n",
            "loss after step 1250 is 0.032176993787288666\n",
            "loss after step 1300 is 0.029302053153514862\n",
            "loss after step 1350 is 0.02406008541584015\n",
            "Epoch: 6\n",
            "Accuracy on test data: 76.56975972388159\n",
            "loss after step 1400 is 0.012758915312588215\n",
            "loss after step 1450 is 0.009753083810210228\n",
            "loss after step 1500 is 0.010040367022156715\n",
            "loss after step 1550 is 0.007449847646057606\n",
            "Epoch: 7\n",
            "Accuracy on test data: 77.2733306783486\n",
            "loss after step 1600 is 0.008818229660391808\n",
            "loss after step 1650 is 0.00648406520485878\n",
            "loss after step 1700 is 0.006052232347428799\n",
            "loss after step 1750 is 0.006960379891097546\n",
            "loss after step 1800 is 0.007957903668284416\n",
            "Epoch: 8\n",
            "Accuracy on test data: 76.72905880791185\n",
            "loss after step 1850 is 0.004191437270492315\n",
            "loss after step 1900 is 0.05001325532793999\n",
            "loss after step 1950 is 0.004783391486853361\n",
            "loss after step 2000 is 0.003024437464773655\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}